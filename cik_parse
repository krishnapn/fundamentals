
## a loop for getting the content files of SEC filings that generates dictionary. CIK list is stored separtely for sanity and clarity. 

import urllib3
from bs4 import BeautifulSoup
import re
import re
import time 
import os
import numpy as np

d={}
for cik in list(finalCIK):
    for i in soup.find_all('a'):
    ## see below how the 'soup' is generated 
        if len(str(i['href']).split('/')) ==6:
            d[cik]= i['href']
            
 ## example how to generate soup for the above loop
 
url = 'https://www.sec.gov/Archives/edgar/data/1249759/'
time.sleep(np.random.randint(0, 5))
   
http = urllib3.PoolManager()
r = http.request('GET', url)
soup=BeautifulSoup(r.data, 'html.parser')
#print(soup.text)
#//*[@id="main-content"]/table/tbody/tr[2]/td[1]/a

soup.find_all(('main-content'))
